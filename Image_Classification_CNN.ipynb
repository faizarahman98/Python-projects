{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RdcFin6rdpb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TITLE: IMAGE CLASSIFICATION FOR CIFAR-10 DATASET USING CNNs**"
      ],
      "metadata": {
        "id": "isZ4XuumZmwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUCTION**"
      ],
      "metadata": {
        "id": "kQafwrasaCPL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Z2FAshZEKX"
      },
      "source": [
        "**INTUTION BEHIND WORKING OF NEURAL NETWORK**\n",
        "\n",
        "It takes several inputs, processes it through multiple neurons from multiple hidden layers, and returns the result using an output layer. This result estimation process is technically known as “Forward Propagation“.\n",
        "\n",
        "Next, we compare the result with actual output. The task is to make the output to the neural network as close to the actual (desired) output. Each of these neurons is contributing some error to the final output. How do you reduce the error?\n",
        "\n",
        "We try to minimize the value/ weight of neurons that are contributing more to the error and this happens while traveling back to the neurons of the neural network and finding where the error lies. This process is known as “Backward Propagation“.\n",
        "\n",
        "In order to reduce this number of iterations to minimize the error, the neural networks use a common algorithm known as “Gradient Descent”, which helps to optimize the task quickly and efficiently.\n",
        "\n",
        "This is how Neural networks work.\n",
        "\n",
        "Nowadays the terms machine learning and artificial neural networks seem to be applied interchangeably. However, when it comes to ML algorithms there is a lot more under the sun than just neural networks. But why would one ever use anything else but neural networks? It is true that neural networks can achieve amazing results to very complex problems. There are some key issues, though. They require big datasets and a lot of computational power, especially the latter isn’t always within one’s reach. And often it doesn’t make a lot of sense to use something as complex as a neural network for rather small and simple problems, where other algorithms are faster and potentially better!\n",
        "\n",
        "**Introduction to CNN**\n",
        "\n",
        "Convolutional Neural Network is a Deep Learning algorithm specially designed for working with Images and videos. It takes images as inputs, extracts and learns the features of the image, and classifies them based on the learned features.\n",
        "\n",
        "This algorithm is inspired by the working of a part of the human brain which is the Visual Cortex. The visual Cortex is a part of the human brain which is responsible for processing visual information from the outside world. It has various layers and each layer has its own functioning i.e each layer extracts some information from the image or any visual and at last all the information received from each layer is combined and the image/visual is interpreted or classified.\n",
        "\n",
        "Similarly, CNN has various filters, and each filter extracts some information from the image such as edges, different kinds of shapes (vertical, horizontal, round), and then all of these are combined to identify the image.\n",
        "\n",
        "Now, the question here can be: Why can’t we use Artificial Neural Networks for the same purpose? This is because there are some disadvantages with ANN:\n",
        "\n",
        "It is too much computation for an ANN model to train large-size images and different types of image channels. The next disadvantage is that it is unable to capture all the information from an image whereas a CNN model can capture the spatial dependencies of the image. Another reason is that ANN is sensitive to the location of the object in the image i.e if the location or place of the same object changes, it will not be able to classify properly.\n",
        "\n",
        "*Input layer*\n",
        "\n",
        "As the name says, it’s our input image and can be Grayscale or RGB. Every image is made up of pixels that range from 0 to 255. We need to normalize them i.e convert the range between 0 to 1 before passing it to the model.\n",
        "\n",
        "*Convolution Layer*\n",
        "\n",
        "The convolution layer is the layer where the filter is applied to our input image to extract or detect its features. A filter is applied to the image multiple times and creates a feature map which helps in classifying the input image.\n",
        "\n",
        "*Pooling Layer*\n",
        "\n",
        "The pooling layer is applied after the Convolutional layer and is used to reduce the dimensions of the feature map which helps in preserving the important information or features of the input image and reduces the computation time.\n",
        "\n",
        "Using pooling, a lower resolution version of input is created that still contains the large or important elements of the input image.\n",
        "\n",
        "The most common types of Pooling are Max Pooling and Average Pooling. The below figure shows how Max Pooling works. Using the Feature map which we got from the above example to apply Pooling. Here we are using a Pooling layer of size 2*2 with a stride of 2.\n",
        "\n",
        "The maximum value from each highlighted area is taken and a new version of the input image is obtained which is of size 2*2 so after applying Pooling the dimension of the feature map has reduced.\n",
        "\n",
        "*Fully Connected Layer*\n",
        "\n",
        "Till now we have performed the Feature Extraction steps, now comes the Classification part. The Fully connected layer (as we have in ANN) is used for classifying the input image into a label. This layer connects the information extracted from the previous steps (i.e Convolution layer and Pooling layers) to the output layer and eventually classifies the input into the desired label."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBJECTIVES**"
      ],
      "metadata": {
        "id": "YKoanNHxZ9Vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, in this analysis our main objective is to perform a image classification for CFIR-10 dataset using convolution neural network."
      ],
      "metadata": {
        "id": "d5_zpWNyaHMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PACKAGES AND LIBRARIES**"
      ],
      "metadata": {
        "id": "s9f-H88eaJ70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the pandas and numpy that will be used to handle the data.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Import package for visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Importing the required libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import utils\n"
      ],
      "metadata": {
        "id": "5lLi4CeAab-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METHODOLOGY**"
      ],
      "metadata": {
        "id": "RthR-G1papSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Description\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Wqr0mf10ar7e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ulBHgvZEKr"
      },
      "source": [
        "**The CIFAR-10 dataset**\n",
        "\n",
        "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PKehuBFjba0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaXbpIPSZEKy",
        "outputId": "56ccd7f4-6632-4a63-ae3e-9da34b831522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Loading the dataset \n",
        "(X_train,y_train),(X_test,y_test)=cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n-2Vf_DBbkWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikk86RfXZEK2",
        "outputId": "f5702521-b369-4f4d-d518-55eb8cb1dc03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before one-hot encoding:  (50000, 1)\n",
            "Shape after one-hot encoding:  (50000, 10)\n"
          ]
        }
      ],
      "source": [
        "#Building the input vector from the 32x32 pixels\n",
        "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "#Normalizing the data to help with the training\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "#One-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = utils.to_categorical(y_train, n_classes)\n",
        "Y_test = utils.to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building & Analysis\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8Hye-KqgcoFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# convolutional layer\n",
        "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "# convolutional layer\n",
        "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# flatten output of conv\n",
        "model.add(Flatten())\n",
        "\n",
        "# hidden layer\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "# output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# training the model for 10 epochs\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22v6MNyzcm4Y",
        "outputId": "3cc3596e-6df3-4a18-9696-7a7be4ba42ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 561s 1s/step - loss: 1.6140 - accuracy: 0.4045 - val_loss: 1.2171 - val_accuracy: 0.5676\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 567s 1s/step - loss: 1.1347 - accuracy: 0.5953 - val_loss: 0.9381 - val_accuracy: 0.6733\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 542s 1s/step - loss: 0.9413 - accuracy: 0.6716 - val_loss: 0.8419 - val_accuracy: 0.7121\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 533s 1s/step - loss: 0.8237 - accuracy: 0.7119 - val_loss: 0.7522 - val_accuracy: 0.7419\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7364 - accuracy: 0.7407Epoch 6/10\n",
            "391/391 [==============================] - 541s 1s/step - loss: 0.6713 - accuracy: 0.7654 - val_loss: 0.7053 - val_accuracy: 0.7569\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 541s 1s/step - loss: 0.6137 - accuracy: 0.7858 - val_loss: 0.6667 - val_accuracy: 0.7700\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 537s 1s/step - loss: 0.5612 - accuracy: 0.8031 - val_loss: 0.6611 - val_accuracy: 0.7710\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 539s 1s/step - loss: 0.5188 - accuracy: 0.8185 - val_loss: 0.6592 - val_accuracy: 0.7747\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 536s 1s/step - loss: 0.4847 - accuracy: 0.8288 - val_loss: 0.6659 - val_accuracy: 0.7701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbad6bb5190>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION**\n"
      ],
      "metadata": {
        "id": "ofQ33uVFdAfQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_rb5u2AZEK4"
      },
      "source": [
        "\n",
        "\n",
        "From the above output we have obtained it can be seen that as the epochs increases - the accuracy tends to increase and finally we get an accuracy of the model as 77.01%. CNNs are often used in image recognition systems. In 2012 an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012. When applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\n",
        "\n",
        "In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n",
        "\n",
        "​"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}